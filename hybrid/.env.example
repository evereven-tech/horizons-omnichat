# =============================================================================
# Horizons OmniChat - Hybrid Deployment Configuration
# =============================================================================
# This file contains all configuration options for the hybrid deployment mode.
# Copy this file to .env and configure the values according to your needs.
#
# Security Best Practices:
# - Use strong, unique passwords for all services
# - Store API keys securely and never commit them to version control
# - Regularly rotate API keys and passwords
# - Use environment-specific configurations for different deployments
# =============================================================================

# =============================================================================
# DATABASE CONFIGURATION
# =============================================================================
# PostgreSQL database configuration for Open WebUI and LiteLLM
# The database stores user data, conversations, and model configurations

POSTGRES_DB=chatbot_db
POSTGRES_USER=chatbot_user
# SECURITY: Change this password in production deployments
POSTGRES_PASSWORD=change_me_in_production

# =============================================================================
# OPEN WEBUI CONFIGURATION
# =============================================================================
# Open WebUI is the frontend interface for interacting with LLM models

# SECURITY: Generate a strong secret key for session management
# You can generate one with: openssl rand -hex 32
WEBUI_SECRET_KEY=change_me_in_production

# Open WebUI version - use specific versions for production stability
WEBUI_VERSION=v0.6.22

# =============================================================================
# OLLAMA CONFIGURATION
# =============================================================================
# Ollama provides local LLM model serving capabilities

# Enable GPU acceleration if you have compatible hardware (NVIDIA GPUs)
# Set to 'true' to enable GPU support, 'false' for CPU-only mode
OLLAMA_USE_GPU=false

# Comma-separated list of models to install automatically
# Popular options: llama3.2:3b, phi4, qwen2.5:7b, mistral:7b
# For coding: codellama:7b, deepseek-coder:6.7b
# Lightweight: tinyllama, phi3:mini
INSTALLED_MODELS=tinyllama,phi4,hdnh2006/salamandra-7b-instruct

# =============================================================================
# AWS/BEDROCK CONFIGURATION
# =============================================================================
# AWS Bedrock provides access to foundation models from various providers
# Configure these if you want to use AWS Bedrock models

# AWS Credentials - obtain from AWS IAM
AWS_ACCESS_KEY_ID=your_access_key_id
AWS_SECRET_ACCESS_KEY=your_secret_access_key
# Optional: for temporary credentials or cross-account access
AWS_SESSION_TOKEN=your_session_token

# AWS Region - choose based on model availability and latency requirements
# Popular regions: us-west-2, us-east-1, eu-west-1
AWS_REGION=us-west-2

# Bedrock Gateway API Key - used for internal authentication
# This is a custom key for the bedrock-gateway service
BEDROCK_API_KEY=123456

# =============================================================================
# LITELLM CORE CONFIGURATION
# =============================================================================
# LiteLLM acts as a unified proxy for multiple LLM providers

# Admin credentials for LiteLLM management interface
UI_USERNAME=admin
# SECURITY: Change this password in production
UI_PASSWORD=change_me_in_production

# Master key for LiteLLM API authentication
# Format: sk-<random_string> (similar to OpenAI API key format)
# SECURITY: Generate a strong key and keep it secure
LITELLM_MASTER_KEY=sk-change_me_in_production

# Salt key for additional security (optional, defaults to master key)
LITELLM_SALT_KEY=sk-change_me_in_production

# =============================================================================
# EXTERNAL LLM PROVIDER API KEYS
# =============================================================================
# Configure API keys for external LLM providers you want to use
# Uncomment and set the appropriate values for each provider you need
#
# Note: All provider keys use the LITELLM_PROVIDER_ prefix for consistency
# Only configure the providers you actually plan to use

# -----------------------------------------------------------------------------
# OpenAI Configuration
# -----------------------------------------------------------------------------
# Provides access to GPT-4o, GPT-4o-mini, DALL-E, and other OpenAI models
# Get your API key from: https://platform.openai.com/api-keys
# Format: sk-<48+ character string>
#LITELLM_PROVIDER_OPENAI_API_KEY=sk-your_openai_api_key_here

# -----------------------------------------------------------------------------
# Anthropic Configuration  
# -----------------------------------------------------------------------------
# Provides access to Claude models via direct API (alternative to Bedrock)
# Get your API key from: https://console.anthropic.com/
# Format: sk-ant-<95+ character string>
#LITELLM_PROVIDER_ANTHROPIC_API_KEY=sk-ant-your_anthropic_api_key_here

# -----------------------------------------------------------------------------
# Mistral AI Configuration
# -----------------------------------------------------------------------------
# Provides access to Mistral Large, Medium, Small, and Codestral models
# Get your API key from: https://console.mistral.ai/
# Format: <32+ character alphanumeric string>
#LITELLM_PROVIDER_MISTRAL_API_KEY=your_mistral_api_key_here

# -----------------------------------------------------------------------------
# xAI Configuration
# -----------------------------------------------------------------------------
# Provides access to Grok models from Elon Musk's xAI
# Get your API key from: https://console.x.ai/
# Format: xai-<64+ character string>
#LITELLM_PROVIDER_XAI_API_KEY=xai-your_xai_api_key_here

# -----------------------------------------------------------------------------
# HuggingFace Configuration
# -----------------------------------------------------------------------------
# Provides access to open-source models hosted on HuggingFace
# Get your API key from: https://huggingface.co/settings/tokens
# Format: hf_<37+ character string>
#LITELLM_PROVIDER_HUGGINGFACE_API_KEY=hf_your_huggingface_api_key_here

# -----------------------------------------------------------------------------
# Cohere Configuration
# -----------------------------------------------------------------------------
# Provides access to Command R+, Command R, and other Cohere models
# Get your API key from: https://dashboard.cohere.ai/api-keys
# Format: <variable length string>
#LITELLM_PROVIDER_COHERE_API_KEY=your_cohere_api_key_here

# =============================================================================
# ADVANCED CONFIGURATION OPTIONS
# =============================================================================
# These options are for advanced users and typically don't need to be changed

# LiteLLM Performance Settings
# Maximum request timeout in seconds
#LITELLM_REQUEST_TIMEOUT=600

# Maximum budget for API calls (optional cost control)
#LITELLM_MAX_BUDGET=1000

# Logging level for LiteLLM (DEBUG, INFO, WARNING, ERROR)
#LITELLM_LOG_LEVEL=INFO

# Enable detailed debugging (useful for troubleshooting)
#LITELLM_DEBUG=false

# =============================================================================
# DEPLOYMENT NOTES
# =============================================================================
# 
# 1. REQUIRED CONFIGURATION:
#    - Set strong passwords for POSTGRES_PASSWORD, WEBUI_SECRET_KEY, UI_PASSWORD
#    - Configure LITELLM_MASTER_KEY with a secure value
#
# 2. PROVIDER SETUP:
#    - You need at least one provider configured (Ollama is always available)
#    - Uncomment and configure API keys for external providers you want to use
#    - AWS Bedrock requires valid AWS credentials and appropriate IAM permissions
#
# 3. SECURITY CONSIDERATIONS:
#    - Never commit this file with real API keys to version control
#    - Use different configurations for development, staging, and production
#    - Regularly rotate API keys and passwords
#    - Monitor API usage and costs for external providers
#
# 4. GETTING STARTED:
#    - Copy this file to .env: cp .env.example .env
#    - Configure at least the required settings above
#    - Run: make hybrid-up
#    - Access the interface at http://localhost:3002
#
# 5. TROUBLESHOOTING:
#    - Run the configuration generator: bin/generate-config-litellm-hybrid.sh
#    - Check service logs: docker-compose logs <service-name>
#    - Validate configuration: bin/generate-config-litellm-hybrid.sh --validate-only
#
# =============================================================================
