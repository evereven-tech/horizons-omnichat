model_list:

  - model_name: "*"                             # all requests where model not in your config go to this deployment
    litellm_params:
      model: openai/*                           # set `openai/` to use the openai route
      api_key: os.environ/OPENAI_API_KEY

  - model_name: llama-3.1-8B-instruct
    litellm_params:
      model: huggingface/meta-llama/Meta-Llama-3.1-8B-Instruct
      api_key: os.environ/HUGGINGFACE_API_KEY

  - model_name: mistral-small-latest
    litellm_params:
      model: mistral/mistral-small-latest
      api_key: os.environ/MISTRAL_API_KEY       # ensure you have `MISTRAL_API_KEY` in your .env

  - model_name: my-model
    litellm_params:
      model: xai/*                              # add xai/ prefix to route as XAI provider
      api_key: os.environ/XAI_API_KEY           # api key to send your model

  - model_name: claude-3
    litellm_params:
      model: bedrock/anthropic.claude-3-5-sonnet-20240620-v1:0
      aws_region_name: $AWS_REGION
