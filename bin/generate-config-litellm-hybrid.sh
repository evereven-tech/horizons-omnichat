#!/bin/bash

# LiteLLM Configuration Generator for Hybrid Deployment
# This script generates LiteLLM configuration YAML files by:
# 1. Discovering available AWS Bedrock models
# 2. Reading external provider API keys from .env
# 3. Generating comprehensive configuration files

set -e

# Colours for output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m' # No Colour

# Configuration
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
PROJECT_ROOT="$(dirname "$SCRIPT_DIR")"
ENV_FILE="${PROJECT_ROOT}/hybrid/.env"
OUTPUT_DIR="${PROJECT_ROOT}/hybrid"
CONFIG_FILE="${OUTPUT_DIR}/litellm_config.yaml"
IMAGE_CONFIG_FILE="${OUTPUT_DIR}/litellm_image_config.yaml"

# Function to print coloured messages
log_info() {
    echo -e "${BLUE}â„¹ï¸  $1${NC}"
}

log_success() {
    echo -e "${GREEN}âœ… $1${NC}"
}

log_warning() {
    echo -e "${YELLOW}âš ï¸  $1${NC}"
}

log_error() {
    echo -e "${RED}âŒ $1${NC}"
}

# Check if .env file exists
if [ ! -f "$ENV_FILE" ]; then
    log_error "Environment file not found: $ENV_FILE"
    exit 1
fi

# Load environment variables
log_info "Loading environment variables from $ENV_FILE"
set -a
source "$ENV_FILE"
set +a

# Check AWS CLI availability
if ! command -v aws &> /dev/null; then
    log_error "AWS CLI is not installed. Please install it first."
    exit 1
fi

# Check AWS credentials
if [ -z "$AWS_REGION" ]; then
    log_error "AWS_REGION is not set in .env file"
    exit 1
fi

log_info "Using AWS Region: $AWS_REGION"

# Function to discover Bedrock text models
discover_bedrock_text_models() {
    log_info "Discovering Bedrock text/chat models in $AWS_REGION..."
    
    # Get inference profiles (cross-region models)
    local inference_profiles=$(aws bedrock list-inference-profiles \
        --region "$AWS_REGION" \
        --query 'inferenceProfileSummaries[?status==`ACTIVE`].[inferenceProfileName,inferenceProfileId,description]' \
        --output json 2>/dev/null || echo "[]")
    
    # Get foundation models (region-specific)
    local foundation_models=$(aws bedrock list-foundation-models \
        --region "$AWS_REGION" \
        --query 'modelSummaries[?modelLifecycle.status==`ACTIVE` && responseStreamingSupported==`true` && contains(inferenceTypesSupported, `ON_DEMAND`) && contains(inputModalities, `TEXT`) && contains(outputModalities, `TEXT`)].[modelName,modelId,providerName]' \
        --output json 2>/dev/null || echo "[]")
    
    echo "$inference_profiles" > /tmp/inference_profiles.json
    echo "$foundation_models" > /tmp/foundation_models.json
}

# Function to discover Bedrock image models
discover_bedrock_image_models() {
    log_info "Discovering Bedrock image generation models in $AWS_REGION..."
    
    local image_models=$(aws bedrock list-foundation-models \
        --region "$AWS_REGION" \
        --query 'modelSummaries[?modelLifecycle.status==`ACTIVE` && contains(outputModalities, `IMAGE`)].[modelName,modelId,providerName]' \
        --output json 2>/dev/null || echo "[]")
    
    echo "$image_models" > /tmp/image_models.json
}

# Function to generate text models YAML
generate_text_config() {
    log_info "Generating LiteLLM text/chat configuration..."
    
    cat > "$CONFIG_FILE" << 'EOF'
# LiteLLM Configuration - Generated by generate-config-litellm-hybrid.sh
# This file is auto-generated. Do not edit manually.
# Generated at: $(date)

model_list:
EOF
    
    # Add Bedrock inference profiles
    if [ -f /tmp/inference_profiles.json ] && [ "$(cat /tmp/inference_profiles.json)" != "[]" ]; then
        echo "  # ============================================" >> "$CONFIG_FILE"
        echo "  # AWS Bedrock Inference Profiles (Cross-Region)" >> "$CONFIG_FILE"
        echo "  # ============================================" >> "$CONFIG_FILE"
        
        jq -r '.[] | "  - model_name: \"\(.[0])\"\n    litellm_params:\n      model: bedrock/\(.[1])\n      aws_region_name: '"$AWS_REGION"'"' /tmp/inference_profiles.json >> "$CONFIG_FILE"
        echo "" >> "$CONFIG_FILE"
    fi
    
    # Add Bedrock foundation models
    if [ -f /tmp/foundation_models.json ] && [ "$(cat /tmp/foundation_models.json)" != "[]" ]; then
        echo "  # ============================================" >> "$CONFIG_FILE"
        echo "  # AWS Bedrock Foundation Models" >> "$CONFIG_FILE"
        echo "  # ============================================" >> "$CONFIG_FILE"
        
        jq -r '.[] | "  - model_name: \"\(.[0])\"\n    litellm_params:\n      model: bedrock/\(.[1])\n      aws_region_name: '"$AWS_REGION"'"' /tmp/foundation_models.json >> "$CONFIG_FILE"
        echo "" >> "$CONFIG_FILE"
    fi
    
    # Add external providers if API keys are present
    echo "  # ============================================" >> "$CONFIG_FILE"
    echo "  # External Provider Models" >> "$CONFIG_FILE"
    echo "  # ============================================" >> "$CONFIG_FILE"
    
    # OpenAI
    if [ ! -z "$LITELLM_PROVIDER_OPENAI_API_KEY" ] && [ "$LITELLM_PROVIDER_OPENAI_API_KEY" != "your_openai_api_key" ]; then
        cat >> "$CONFIG_FILE" << EOF
  
  # OpenAI Models
  - model_name: "gpt-4o"
    litellm_params:
      model: openai/gpt-4o
      api_key: os.environ/LITELLM_PROVIDER_OPENAI_API_KEY
      
  - model_name: "gpt-4o-mini"
    litellm_params:
      model: openai/gpt-4o-mini
      api_key: os.environ/LITELLM_PROVIDER_OPENAI_API_KEY
      
  - model_name: "gpt-4-turbo"
    litellm_params:
      model: openai/gpt-4-turbo
      api_key: os.environ/LITELLM_PROVIDER_OPENAI_API_KEY
EOF
    fi
    
    # Mistral
    if [ ! -z "$LITELLM_PROVIDER_MISTRAL_API_KEY" ] && [ "$LITELLM_PROVIDER_MISTRAL_API_KEY" != "your_mistral_api_key" ]; then
        cat >> "$CONFIG_FILE" << EOF
  
  # Mistral Models
  - model_name: "mistral-large"
    litellm_params:
      model: mistral/mistral-large-latest
      api_key: os.environ/LITELLM_PROVIDER_MISTRAL_API_KEY
      
  - model_name: "mistral-medium"
    litellm_params:
      model: mistral/mistral-medium-latest
      api_key: os.environ/LITELLM_PROVIDER_MISTRAL_API_KEY
      
  - model_name: "mistral-small"
    litellm_params:
      model: mistral/mistral-small-latest
      api_key: os.environ/LITELLM_PROVIDER_MISTRAL_API_KEY
      
  - model_name: "codestral"
    litellm_params:
      model: mistral/codestral-latest
      api_key: os.environ/LITELLM_PROVIDER_MISTRAL_API_KEY
EOF
    fi
    
    # Anthropic (if using direct API, not Bedrock)
    if [ ! -z "$LITELLM_PROVIDER_ANTHROPIC_API_KEY" ] && [ "$LITELLM_PROVIDER_ANTHROPIC_API_KEY" != "your_anthropic_api_key" ]; then
        cat >> "$CONFIG_FILE" << EOF
  
  # Anthropic Models (Direct API)
  - model_name: "claude-3-opus"
    litellm_params:
      model: anthropic/claude-3-opus-20240229
      api_key: os.environ/LITELLM_PROVIDER_ANTHROPIC_API_KEY
      
  - model_name: "claude-3-sonnet"
    litellm_params:
      model: anthropic/claude-3-sonnet-20240229
      api_key: os.environ/LITELLM_PROVIDER_ANTHROPIC_API_KEY
EOF
    fi
    
    # HuggingFace
    if [ ! -z "$LITELLM_PROVIDER_HUGGINGFACE_API_KEY" ] && [ "$LITELLM_PROVIDER_HUGGINGFACE_API_KEY" != "your_huggingface_api_key" ]; then
        cat >> "$CONFIG_FILE" << EOF
  
  # HuggingFace Models
  - model_name: "llama-3.1-8b"
    litellm_params:
      model: huggingface/meta-llama/Meta-Llama-3.1-8B-Instruct
      api_key: os.environ/LITELLM_PROVIDER_HUGGINGFACE_API_KEY
      
  - model_name: "mixtral-8x7b"
    litellm_params:
      model: huggingface/mistralai/Mixtral-8x7B-Instruct-v0.1
      api_key: os.environ/LITELLM_PROVIDER_HUGGINGFACE_API_KEY
EOF
    fi
    
    # XAI (Grok)
    if [ ! -z "$LITELLM_PROVIDER_XAI_API_KEY" ] && [ "$LITELLM_PROVIDER_XAI_API_KEY" != "your_xai_api_key" ]; then
        cat >> "$CONFIG_FILE" << EOF
  
  # XAI Models
  - model_name: "grok-beta"
    litellm_params:
      model: xai/grok-beta
      api_key: os.environ/LITELLM_PROVIDER_XAI_API_KEY
EOF
    fi
    
    # Add LiteLLM settings
    cat >> "$CONFIG_FILE" << EOF

# ============================================
# LiteLLM Settings
# ============================================
litellm_settings:
  drop_params: true
  success_callback: []  # Add "langfuse" for observability if needed

# ============================================
# General Settings
# ============================================
general_settings:
  master_key: os.environ/LITELLM_MASTER_KEY
  database_url: postgresql://\${POSTGRES_USER}:\${POSTGRES_PASSWORD}@webui-db:5432/\${POSTGRES_DB}
  
router_settings:
  routing_strategy: simple-shuffle  # Options: simple-shuffle, least-busy, usage-based-routing
  model_group_alias: {}
  num_retries: 3
  timeout: 600  # 10 minutes
  max_tokens: 4096
  fallbacks: []
EOF
}

# Function to generate image models YAML
generate_image_config() {
    log_info "Generating LiteLLM image generation configuration..."
    
    cat > "$IMAGE_CONFIG_FILE" << 'EOF'
# LiteLLM Image Generation Configuration - Generated by generate-config-litellm-hybrid.sh
# This file is auto-generated. Do not edit manually.
# Generated at: $(date)

model_list:
EOF
    
    # Add Bedrock image models
    if [ -f /tmp/image_models.json ] && [ "$(cat /tmp/image_models.json)" != "[]" ]; then
        echo "  # ============================================" >> "$IMAGE_CONFIG_FILE"
        echo "  # AWS Bedrock Image Generation Models" >> "$IMAGE_CONFIG_FILE"
        echo "  # ============================================" >> "$IMAGE_CONFIG_FILE"
        
        jq -r '.[] | "  - model_name: \"\(.[0])\"\n    litellm_params:\n      model: bedrock/\(.[1])\n      aws_region_name: '"$AWS_REGION"'"' /tmp/image_models.json >> "$IMAGE_CONFIG_FILE"
        echo "" >> "$IMAGE_CONFIG_FILE"
    fi
    
    # Add OpenAI image models if API key present
    if [ ! -z "$LITELLM_PROVIDER_OPENAI_API_KEY" ] && [ "$LITELLM_PROVIDER_OPENAI_API_KEY" != "your_openai_api_key" ]; then
        cat >> "$IMAGE_CONFIG_FILE" << EOF
  
  # OpenAI Image Models
  - model_name: "dall-e-3"
    litellm_params:
      model: openai/dall-e-3
      api_key: os.environ/LITELLM_PROVIDER_OPENAI_API_KEY
      
  - model_name: "dall-e-2"
    litellm_params:
      model: openai/dall-e-2
      api_key: os.environ/LITELLM_PROVIDER_OPENAI_API_KEY
EOF
    fi
    
    # Add settings
    cat >> "$IMAGE_CONFIG_FILE" << EOF

# ============================================
# LiteLLM Settings
# ============================================
litellm_settings:
  drop_params: true

# ============================================
# General Settings
# ============================================
general_settings:
  master_key: os.environ/LITELLM_MASTER_KEY
EOF
}

# Main execution
main() {
    log_info "Starting LiteLLM configuration generation for hybrid deployment"
    
    # Test AWS connectivity
    log_info "Testing AWS connectivity..."
    if aws sts get-caller-identity --region "$AWS_REGION" &>/dev/null; then
        log_success "AWS credentials are valid"
    else
        log_warning "AWS credentials may not be valid or AWS CLI is not configured properly"
        log_warning "Continuing without Bedrock model discovery..."
    fi
    
    # Discover models
    discover_bedrock_text_models
    discover_bedrock_image_models
    
    # Generate configurations
    generate_text_config
    generate_image_config
    
    # Clean up temporary files
    rm -f /tmp/inference_profiles.json /tmp/foundation_models.json /tmp/image_models.json
    
    # Summary
    log_success "Configuration files generated successfully!"
    echo ""
    log_info "Generated files:"
    echo "  ðŸ“„ $CONFIG_FILE"
    echo "  ðŸ“„ $IMAGE_CONFIG_FILE"
    echo ""
    
    # Count models
    if [ -f "$CONFIG_FILE" ]; then
        local text_models=$(grep -c "model_name:" "$CONFIG_FILE" || echo "0")
        log_info "Text/Chat models configured: $text_models"
    fi
    
    if [ -f "$IMAGE_CONFIG_FILE" ]; then
        local image_models=$(grep -c "model_name:" "$IMAGE_CONFIG_FILE" || echo "0")
        log_info "Image generation models configured: $image_models"
    fi
    
    echo ""
    log_info "To use these configurations:"
    echo "  1. Ensure your .env file has the correct API keys"
    echo "  2. Run: cd hybrid && docker-compose up -d"
    echo "  3. LiteLLM will be available at http://localhost:4000"
}

# Run main function
main "$@"
